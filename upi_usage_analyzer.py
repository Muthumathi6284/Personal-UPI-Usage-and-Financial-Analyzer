# -*- coding: utf-8 -*-
"""UPI Usage Analyzer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14RS7Il3glTnyFyTQnZfG_iocOuhZu6LY
"""

!pip install pdfplumber pandas

from google.colab import files
uploaded = files.upload()   # select your PDF file

import pdfplumber

def extract_text(pdf_path):
    text = ""
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            page_text = page.extract_text()
            if page_text:
                text += page_text + "\n"
    return text

raw_text = extract_text("mPassbook_CA-510909010204363_1758262246131.pdf")
print(raw_text[:1000])  # preview first 1000 chars

import re
import pandas as pd

def parse_transactions(text):
    data = []
    lines = text.split("\n")

    pattern = re.compile(
        r"(\d{2}/\d{2}/\d{4})\s+(.*?)\s+(\d{1,3}(?:,\d{3})*(?:\.\d{2})?)\s+(\d{1,3}(?:,\d{3})*(?:\.\d{2})?)?"
    )

    for line in lines:
        m = pattern.search(line)
        if m:
            date = m.group(1)
            desc = m.group(2).strip()
            debit = m.group(3) if "TO" in line else None
            credit = m.group(3) if "BY" in line else None
            if debit:
                amount = -float(debit.replace(",", ""))
            elif credit:
                amount = float(credit.replace(",", ""))
            else:
                amount = None

            data.append({
                "Date": date,
                "Description": desc,
                "Amount": amount,
                "Category": "Uncategorized"
            })
    return pd.DataFrame(data)

df = parse_transactions(raw_text)
df.head(10)

df.to_csv("upi_transactions.csv", index=False)
df.to_json("upi_transactions.json", orient="records", indent=2)

from google.colab import files
files.download("upi_transactions.csv")

import pandas as pd

# Load your parsed bank statement
df = pd.read_csv("upi_transactions.csv")

# Ensure Date column is datetime
df["Date"] = pd.to_datetime(df["Date"], errors="coerce")

# Preview
df.head()

category_keywords = {
    "Food": ["zomato", "swiggy", "hotel", "restaurant", "bakery"],
    "Travel": ["uber", "ola", "rapido", "irctc", "bus", "train", "flight", "airlines"],
    "Groceries": ["bigbasket", "grofers", "reliance fresh", "more supermarket", "dmart"],
    "Utilities": ["electricity", "gas", "water bill", "recharge", "dth", "bsnl", "airtel", "jio"],
    "Shopping": ["amazon", "flipkart", "myntra", "ajio"],
    "Income": ["salary", "credited", "neft", "rtgs", "imps", "payment received"],
    "Others": []
}

def categorize(description):
    desc = str(description).lower()
    for category, keywords in category_keywords.items():
        for kw in keywords:
            if kw in desc:
                return category
    return "Others"

df["Category"] = df["Description"].apply(categorize)

# Check sample
df[["Description", "Category"]].head(20)

import re

def clean_description(desc):
    desc = str(desc)
    # remove long UPI refs like /123456789/
    desc = re.sub(r'/\d{6,}/', ' ', desc)
    # replace multiple spaces
    desc = re.sub(r'\s+', ' ', desc).strip()
    return desc

df["Clean_Description"] = df["Description"].apply(clean_description)
df[["Description","Clean_Description"]].head(10)

from transformers import pipeline

classifier = pipeline("zero-shot-classification", model="facebook/bart-large-mnli")

candidate_labels = ["Food", "Travel", "Groceries", "Utilities", "Shopping", "Income", "Other"]

def categorize(desc):
    result = classifier(desc, candidate_labels)
    return result["labels"][0]

df["Category"] = df["Clean_Description"].apply(categorize)
df[["Clean_Description","Category"]].head(10)

from transformers import pipeline

classifier = pipeline("zero-shot-classification", model="facebook/bart-large-mnli")

candidate_labels = ["Food", "Travel", "Groceries", "Utilities", "Shopping", "Income", "Other"]

def categorize(desc):
    result = classifier(desc, candidate_labels)
    return result["labels"][0]

df["Category"] = df["Clean_Description"].apply(categorize)
df[["Clean_Description","Category"]].head(10)

def detect_wasteful(row):
    if row["Category"] in ["Food","Shopping","Travel"] and abs(row["Amount"]) < 500:
        return 1   # Wasteful
    return 0       # Not Wasteful

df["Wasteful"] = df.apply(detect_wasteful, axis=1)
df[["Clean_Description","Category","Amount","Wasteful"]].head(15)

print("Wasteful %:", (df["Wasteful"]=="Wasteful").mean()*100)

print("\nWasteful transactions:")
print(df[df["Wasteful"]=="Wasteful"].head(10))

from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import torch.nn.functional as F
0
model_name = "facebook/bart-large-mnli"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

def entailment_score(premise, hypothesis):
    # Encode the text pair
    inputs = tokenizer(premise, hypothesis, return_tensors="pt", truncation=True)
    outputs = model(**inputs)
    logits = outputs.logits
    # Order: ['contradiction', 'neutral', 'entailment']
    entail_contradiction_logits = logits[:, [0, 2]]
    probs = F.softmax(entail_contradiction_logits, dim=1)
    return probs[:, 1].item()  # probability of entailment

candidate_labels = ["Food", "Travel", "Groceries", "Utilities", "Shopping", "Income", "Other"]

def categorize_pair(desc):
    scores = {}
    for label in candidate_labels:
        hypothesis = f"This transaction is about {label}."
        score = entailment_score(desc, hypothesis)
        scores[label] = score
    best_label = max(scores, key=scores.get)   # pick label with highest score
    return best_label, scores

df["Category_Pair"], df["Category_Scores"] = zip(*df["Clean_Description"].apply(categorize_pair))

!pip install gradio

import gradio as gr

candidate_labels = ["Food", "Travel", "Groceries", "Utilities", "Shopping", "Income", "Other"]

# Categorization with text-pair modeling
def categorize_pair(desc):
    scores = {}
    for label in candidate_labels:
        hypothesis = f"This transaction is about {label}."
        score = entailment_score(desc, hypothesis)
        scores[label] = score
    best_label = max(scores, key=scores.get)
    return best_label

# Wasteful / Not Wasteful classification
def wasteful_classification(desc):
    hypotheses = {
        "Wasteful": "This transaction is a wasteful expense.",
        "Not Wasteful": "This transaction is not a wasteful expense."
    }
    scores = {label: entailment_score(desc, hypo) for label, hypo in hypotheses.items()}
    return max(scores, key=scores.get)

# Gradio function
def analyze_transaction(desc, amount):
    category = categorize_pair(desc)
    wasteful = wasteful_classification(desc)
    return category, wasteful

# Build Gradio Interface
demo = gr.Interface(
    fn=analyze_transaction,
    inputs=[gr.Textbox(label="Transaction Description"),
            gr.Number(label="Amount")],
    outputs=[gr.Label(label="Predicted Category"),
             gr.Label(label="Wasteful / Not Wasteful")],
    title="ðŸ’³ UPI Analyzer (Text Pair Modeling)",
    description="Classify bank transactions into categories and detect wasteful spends using text-pair modeling."
)

demo.launch()